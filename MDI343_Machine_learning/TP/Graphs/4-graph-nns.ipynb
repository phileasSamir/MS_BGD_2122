{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDI343\n",
    "## Lab on real-world graph analysis -- graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to get a feeling of real-world graphs. For information on the `scikit-network` library, [the documentation is handy](https://scikit-network.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1+cu102'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sknetwork as skn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to plot the inverse cumulative distribution\n",
    "def ccdf(values):\n",
    "    x = []\n",
    "    y = []\n",
    "    values = sorted(values)\n",
    "\n",
    "    # First make dist\n",
    "    dist = [(key, len(list(group))) for key, group in groupby(values)]\n",
    "\n",
    "    # Then compute inverse cumulative\n",
    "    total = 1.0\n",
    "    for (val, count) in dist:\n",
    "        x.append(val)\n",
    "        y.append(total)\n",
    "        total -= count/len(values)\n",
    "    return x, y\n",
    "\n",
    "# Util function to return the distribution of values\n",
    "def dist(values):\n",
    "    values = sorted(values)\n",
    "\n",
    "    # First make dist\n",
    "    dist = [(key, len(list(group))) for key, group in groupby(values)]\n",
    "    \n",
    "    return [x[0] for x in dist], [x[1] for x in dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on 2 graphs induced by the [Vital articles of Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Level/4), a selection of about 10,000 articles of the English Wikipedia:\n",
    "* the directed graph of hyperlinks between these articles,\n",
    "* the bipartite graph between articles and (stemmed) words used in their summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing files...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['names_labels_hierarchy', 'labels_hierarchy', 'names_col', 'adjacency', 'biadjacency', 'meta', 'names_labels', 'names', 'labels'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = skn.data.load_netset('wikivitals')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# graph of links\n",
    "adjacency = dgl.from_scipy(data.adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of words\n",
    "biadjacency = dgl.bipartite_from_scipy(data.biadjacency, \"articles\", \"words\", \"occurrence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article names\n",
    "names = data.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arts', 'Biological and health sciences', 'Everyday life',\n",
       "       'Geography', 'History', 'Mathematics', 'People',\n",
       "       'Philosophy and religion', 'Physical sciences',\n",
       "       'Society and social sciences', 'Technology'], dtype='<U30')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article categories\n",
    "categories = data.names_labels\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alexis', 'Cairo', 'subordinate', ..., 'parody', 'martyrdom',\n",
       "       'Wyler'], dtype='<U26')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words\n",
    "words = data.names_col\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index = {name:i for i, name in enumerate(names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words, num_articles = biadjacency.num_dst_nodes(), biadjacency.num_src_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "For the 2 graphs:\n",
    "* Separate the data into training and validation sets\n",
    "* Fill the code to implement a GCN with the deep graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "# Change for Wikipedia\n",
    "num_features = 1433\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, features_in, features_out):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(features_in, features_out)\n",
    "        \n",
    "    def forward(self, g, feature):\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata[\"h\"]\n",
    "            return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNet, self).__init__()\n",
    "        self.layer1 = GCNLayer(num_features, 16)\n",
    "        self.layer2 = GCNLayer(16, num_classes)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = self.layer2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNet(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = GCNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use some more common dataset, just to get a hang of how things work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = dgl.from_networkx(data.graph)\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 00000 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00001 | Loss 0.0000 | Accuracy on test 0.7200\n",
      "Epoch 00002 | Loss 0.0000 | Accuracy on test 0.7200\n",
      "Epoch 00003 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00004 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00005 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00006 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00007 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00008 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "Epoch 00009 | Loss 0.0000 | Accuracy on test 0.7210\n",
      "\n",
      "|-----------|\n",
      "| TEST      |\n",
      "| ACCURACY: |\n",
      "| 0.7210    |\n",
      "|-----------|\n",
      "(\\__/) ||\n",
      "(•ㅅ•) ||\n",
      "/ 　 づ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "\n",
    "# Add edges between each node and itself to preserve old node representations\n",
    "g.add_edges(g.nodes(), g.nodes())\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy on test {:.4f}\".format(\n",
    "            epoch, loss.item(), acc))\n",
    "    \n",
    "print(\"\"\"\n",
    "|-----------|\n",
    "| TEST      |\n",
    "| ACCURACY: |\n",
    "| {:.4f}    |\n",
    "|-----------|\n",
    "(\\__/) ||\n",
    "(•ㅅ•) ||\n",
    "/ 　 づ\n",
    "\"\"\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    " * Separate our dataset into train and test (you just need to define a `train_mask` and a `test_mask`, which are Boolean vectors)\n",
    " * Adapt the wikipedia dataset to run our GCN model\n",
    " * Use the GCN to \"find out\" the article category of the articles in the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for features, you can use a vector of 0/1 indicating the absence/presence of a word in a given article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikivitals_data():\n",
    "    % \n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
